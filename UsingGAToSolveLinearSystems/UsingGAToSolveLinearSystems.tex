\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{amsthm}
\usepackage{graphicx}
%\usepackage{float}

\title{On The Use of Geometric Algebra\\to Solve Systems of Linear Equations}
\author{Spencer T. Parkin}

\newcommand{\G}{\mathbb{G}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\grade}{\mbox{grade}}

\begin{document}
\maketitle

A common equation that arrises in science and engineering is as follows.
\begin{equation}
Mx=y
\end{equation}
Here, $M$ is a square matrix, and $x,y$ are vectors.  If $M$ is non-singular, the solution is simply
\begin{equation}\label{equ_matrix_algebra_solved}
x=M^{-1}y.
\end{equation}
This is quite satisfactory in the sense that matrix inverses are well understood, but the contention here is that
there is still something left wanting.  You see, a general matrix inverse requires a row-reduction algorithm, or
a formulation in terms of an adjugate matrix that seems to have unknown motivation.  In contrast, what we'll show in this paper is that a
system of linear equations has a straight-forward and clear formulation in terms of geometric algebra.

To that end, let $x,y$ be vectors taken from an $n$-dimensional vector space $\V$, and let the rows of $M$
be denoted by vectors $m_1$, $m_2$, and so on, to $m_n$, each taken from the same vector space $\V$.
Having done so, our system of linear equations can be expressed as
\begin{equation}\label{equ_linear_system}
\left\{
\begin{array}{cc}
m_1\cdot x &= y\cdot e_1 \\
m_2\cdot x &= y\cdot e_2 \\
\vdots & \vdots \\
m_n\cdot x &= y\cdot e_n
\end{array}
\right.
\end{equation}
Yes, this is now looking like geometric algebra; and no, it doesn't look as nice as matrix algebra,
but bare with me.  Note that $\{e_i\}_{i=1}^n$ are an orthonormal set of basis vectors for $\V$.

For a vector $a\in\V$ and a blade $B$ of grade $k\leq n$, we will now make use of the following identity.
\begin{equation}
a\cdot B = -\sum_{i=1}^k(-1)^i (a\cdot b_i) \bigwedge_{j=1,j\neq i}^k b_i,
\end{equation}
where $B=\bigwedge_{i=1}^k b_i$, each $b_i\in\V$.  With this in hand, we may return to our
system of linear equations, and write
\begin{equation}
x\cdot\bigwedge_{i=1}^n m_i = -\sum_{i=1}^n (-1)^i (x\cdot m_i)\bigwedge_{j=1,j\neq i}^n m_i.
\end{equation}
Interestingly, although our unknown vector $x$ is used on both sides of the equation here, the right-hand side
is completely known.  To illustrate this, let's rewrite it with each equation in \eqref{equ_linear_system} substituted.
\begin{equation}\label{equ_substituted}
x\cdot\bigwedge_{i=1}^n m_i = -\sum_{i=1}^n (-1)^i (y\cdot e_i)\bigwedge_{j=1,j\neq i}^n m_i
\end{equation}
What's more, realize that
\begin{equation}
x\wedge\bigwedge_{i=1}^n m_i = 0,
\end{equation}
since clearly in $(n+1)$-blade formed from $\V$ is necessarily zero.  Thus, the inner product of \eqref{equ_substituted}
can be changed to a geometric product.  This allows us to isolate $x$ on the left-hand side by multiplying both sides
by an inverse as follows.
\begin{equation}
x = \left(-\sum_{i=1}^n (-1)^i (y\cdot e_i)\bigwedge_{j=1,j\neq i}^n m_i\right)\left(\bigwedge_{i=1}^n m_i\right)^{-1}
\end{equation}
Some notation now might help to clean this up.  We'll let $M=\bigwedge_{i=1}^n m_i$, and
\begin{equation}
M_j=\bigwedge_{i=1,i\neq j}^n m_i,
\end{equation}
as well as $y_i=y\cdot e_i$.  Our equation then becomes
\begin{equation}
x = \left(-\sum_{i=1}^n(-1)^i y_i M_i\right)M^{-1}.
\end{equation}
But since the grade of $M$ is $n$, we also know that $M$ is a psuedo-scalar of the geometric algebra generated by $\V$.
That is, $M$ is a scalar multiple of the unit psuedo-scalar $I$.  Specifically, we have
\begin{equation}
M^{-1} = \frac{1}{|M|}I^{-1} = \frac{(-1)^{n(n-1)/2}}{|M|}I.
\end{equation}
Therefore, if we let $M_i^{*}=M_iI$ denote the dual of $M_i$, then we finally arrive at
\begin{equation}\label{equ_ga_solved}
x = \frac{-(-1)^{n(n-1)/2}}{|M|}\sum_{i=1}^n (-1)^i y_i M_i^{*}.
\end{equation}
The reader may recognize here $|M|$ as being the determinant of $M$.  Equation \eqref{equ_ga_solved}
may be a bit more complicated than equation \eqref{equ_matrix_algebra_solved}, but it doesn't leave
any question about how to actually calculate $x$.  Interestingly, we see $x$ here is a certain linear
combination of the duals of each $M_i$.  This lets us at least try to visualize the geometric significance
of the result.

Let's now consider how we might use our result to find the general inverse of a linear transformation.
First, let's express the original linear transformation as follows.
\begin{equation}
f(x) = \sum_{i=1}^n (m_i\cdot x)e_i
\end{equation}
If we now let
\begin{equation}
a_i = \frac{-(-1)^{i+n(n-1)/2}}{|M|}M_i^{*},
\end{equation}
then we may write
\begin{equation}
f^{-1}(x)=\sum_{i=1}^n (x\cdot e_i)a_i.
\end{equation}
Here, we almost have the inverse function in the same form as the original function.
To get it there, let $A$ be a matrix with each row being $a_i$, and then $B$ be a
matrix with each column being $b_i$.  Now let $B$ be the transpose of $A$.
We then have
\begin{equation}
f^{-1}(x)=\sum_{i=1}^n (b_i\cdot x)e_i.
\end{equation}
And there we have it.  This is still not quite satisfactory, though, because we still had to use some
matrix trickery to get the result, but it's at least directly computable with minimal need for algorithms
beyond basic arithmatic.

\end{document}