\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{float}

\title{A Quick Guide To Geometric Algebra}
\author{Spencer T. Parkin}

\newcommand{\G}{\mathbb{G}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\B}{\mathbb{B}}
\newcommand{\nvao}{o}
\newcommand{\nvai}{\infty}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{identity}{Identity}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{result}{Result}[section]

\begin{document}
\maketitle

Geometric algebra (GA) is easy and you can be up and running with it quickly!  The key is to
simply understand the basic definitions and identities of the algebra.  From these a number
of simple results can be deduced.  We begin with a few simple definitions from linear algebra (LA).

\section{Definitions of LA}

Linear algebra is the study of linear functions defined on linear spaces.  We will not need
to define linear functions here, but we will need to define linear spaces.
\begin{definition}\label{def_vec_space}
A linear space $\V$ is a set of elements, where each element is called a vector.  It does not stand on its own.
Associated with every vector space $\V$ is a set of scalars, usually the set of real numbers $\R$,
such that for every scalar $\lambda\in\R$ and any two vectors $a,b\in\V$, we have
\begin{align}
\lambda a &\in \V\label{equ_vec_scalar_mul} \\
a+b &\in\V\label{equ_vec_addition}
\end{align}
We define vector addition to be commutative and associative.
\end{definition}
Linear spaces are more commonly referred to as vector spaces, and so we will no longer
use the term linear space.

Notice that equation \eqref{equ_vec_scalar_mul} defines the relationship between the
vector space $\V$ and the real numbers $\R$ associated with it.

\begin{definition}
A linear combination of a set of $k$ vectors $\{v_i\}_{i=1}^k$, taken from $\V$, is
a vector $v\in\V$, written as
\begin{equation}
v = \lambda_1 v_1 + \lambda_2 v_2 + \dots + \lambda_k v_k,
\end{equation}
where $\{\lambda_i\}_{i=1}^k$ is a set of $k$ scalars taken from $\R$.
\end{definition}

We now introduce the idea of linear dependence and linear independence.

\begin{definition}
A set of $k$ vectors $\{v_i\}_{i=1}^k$, taken from $\V$, is said to be linearly
independent if there does not exist a vector $v\in\{v_i\}_{i=1}^k$ such that
$v$ is any linear combination of the remaining $k-1$ vectors in $\{v_i\}_{i=1}^k-\{v\}$.
\end{definition}

In other words, no one of the $k$ vectors in $\{v_i\}_{i=1}^k$ can be
written as a linear combination of the other $k-1$ vectors.  Intuitively, if
we were to attach a geometric interpretation to the vectors in $\V$, then
each vector in $\{v_i\}_{i=1}^k$, if $\{v_i\}_{i=1}^k$ is a linearly independent set, points
in a direction that is a newly unexplored dimension.  If you can grasp the
definition of linear independence, then the definition of linear dependence
is a snap.

\begin{definition}
A set of $k$ vectors $\{v_i\}_{i=1}^k$, taken from $\V$ is said to be linearly
dependent if and only if it is not linearly independent.
\end{definition}

This means that for such a set of $k$ vectors, we can always find a $v\in\{v_i\}_{i=1}^k$
such that $v$ is a linear combination of the remaining $k-1$ vectors.

Understanding the concepts of linear independence and linear dependence is
critical to an understanding of geometric algebra.

\begin{definition}
The dimension of a vector space $\V$, denoted $\dim\V$, is the largest
possible set of $k$ vectors we can take from $\V$ that is also a linearly
independent set.
\end{definition}

If such a set of vectors does not exist, then the vector space has
infinitely many dimensions.  We will only concern ourselves here in this
guide with finite-dimensional vector spaces.

Given a linearly independent set $\{v_i\}_{i=1}^k$ of $k$ vectors, where $k=\dim\V$,
we can describe $\V$ is the set of all linear combinations of the vectors in $\{v_i\}_{i=1}^k$.
In this way, we refer to $\{v_i\}_{i=1}^k$ as a basis for the vector space $\V$.
Any linearly independent set of vectors taken from $\V$ forms a basis for some
vector sub-space of $V$.  The set of all linear combinations of such a set is
what we refer to as the span of that set, and we can say that a vector sub-space
is spanned by any given basis of that space.

We are now ready to move on to definitions of geometric algebra.

\section{Definitions of GA}

We begin by simultaneously introducing the concept of a blade and a new kind of
product, called the outer product, for vectors, which is not closed in $\V$.  This means
that, unlike what we can say about vector addition, the outer product of two vectors is not a vector.
\begin{definition}\label{def_blade}
A non-zero blade $B$ of grade $k$ is the outer product of the $k$ vectors taken
from a linearly independent set $\{v_i\}_{i=1}^k$, and may be written as
\begin{equation}\label{equ_blade}
B = v_1\wedge v_2\wedge\dots\wedge v_k.
\end{equation}
A blade is considered zero otherwise.  That is, the outer product of $k$
linearly dependent vectors is zero.  The outer product is defined as
anti-commutative, associative, and left and right distributive over addition.
\end{definition}

We will talk about what anti-commutative means later on.

Notice that blades of grade 1 are simply vectors.  Blades of higher grade are simply
a higher-dimensional generalization of the idea of a vector.  We will leave the geometric
interpretation of such blades open to the reader for now, and proceed with our treatment of
blades from a purely algebraic stand-point.

Recall scalar-vector multiplication in equation \eqref{equ_vec_scalar_mul}.
We similarly allow such a multiplication for blades.  Call it scalar-blade multiplication.
Realize that a scalar distributes to one and only one vector in the product of
equation \eqref{equ_blade}.  That is, for any scalar $\lambda\in\R$,
we have
\begin{equation}
\lambda B = v_1\wedge v_2\wedge\dots\wedge \lambda v_i\wedge\dots\wedge v_k,
\end{equation}
where $1\leq i\leq k$, the choice of $i$ here being complete arbitrary.

It is important to realize at this point that, by Definition~\ref{def_blade}, a blade
may be said to represent a vector sub-space of $\V$.  It represents a little more
than that, but let us focus on this idea exclusively.  Given a non-zero blade $B$ of the
form in equation \eqref{equ_blade}, we can take the $k$ vectors in the
set $\{v_k\}_{i=1}^k$ as a basis for a vector sub-space of $\V$ of
dimension $k$.  In fact, every basis for this vector sub-space, when put
in an outer product of the form \eqref{equ_blade}, is, by definition,
equal to $B$, up to scale.  That is, if $\{b_i\}_{i=1}^k$ is any other
basis for the vector sub-space represented by $B$ in equation \eqref{equ_blade},
then there exists a scalar $\lambda\in\R$, such that
\begin{equation}
B = \lambda b_1\wedge b_2\wedge\dots\wedge b_k.
\end{equation}

Continuing with this idea of blades as being representative of vector sub-spaces
of $\V$, we can define the following notation.
\begin{definition}[Membership in a vector sub-space of $\V$]
For any vector $v\in\V$ and a non-zero blade $B$, we say that
\begin{equation}
\mbox{$v\in B$ if and only if $v\wedge B=0$,}
\end{equation}
which is to mean that $v$ is in the vector sub-space of $\V$ represented by $B$
if and only if $v\wedge B=0$.
\end{definition}
It is easy to see that this definition makes sense.  If $v\wedge B=0$, and if $\{v_i\}_{i=1}^k$
is any linearly independent set such that equation \eqref{equ_blade} is
satisfied, then $v\wedge B=0$ if and only if $\{v\}\cup\{v_i\}_{i=1}^k$
is a linearly dependent set.  It follows that $v$ is a linear combination of
the vectors in $\{v_i\}_{i=1}^k$.

\subsection{Blade Addition}

We now extend addition of vectors to addition of blades.  The sum of blades
of various grades are just that.  They do not combine into anything new.
Think of this like adding real and imaginary numbers.  They don't combine
into anything new, but are simply toted about as a pair.  Letting scalars
be what we'll call blades of grade zero, it is now clear that we can form
what we'll call a multivector, the general element of a geometric algebra,
as an element of the form
\begin{equation}
B_0 + B_1 + \dots + B_k,
\end{equation}
where each $B_i$ is a blade of grade $i$.  Clearly $k\leq\dim\V$, since
any blade of grade greater than $k$ must be zero by Definition~\ref{def_blade} above.
To see this, realize that if $k=\dim\V$, then for all $v\in\V$, we have $v\in B_k$.  The
grade $k$, therefore, is in this case the largest possible grade.

Just as vectors add as vectors, and scalars add as scalars, it is tempting to assume
that blades of any grade $k$ will add as blades of that grade.  This is not necessarily true,
and forces us to introduce the notion of $k$-vectors.
\begin{definition}
A $k$-vector is the sum of one or more blades of grade $k$.
\end{definition}
From this definition, it is clear that all $k$-blades are simply $k$-vectors.
The opposite, however, is not generally true.  That is, not all $k$-vectors are
$k$-blades.  To see why, we must study the idea of adding blades of the
same grade $k$.

Let $A$ and $B$ be non-zero blades of grade $k$, and consider the $k$-vector
$C$ given as
\begin{equation}
C = A + B,
\end{equation}
the sum of $A$ and $B$.  Now, by virtue of being blades, each of
$A$ and $B$ have a factorization in terms of a
linearly independent set $\{a_i\}_{i=1}^k$ or $\{b_i\}_{i=1}^k$,
respectively.  That is, we may write each of $A$ and $B$ as
\begin{align}
A &= a_1\wedge a_2\wedge\dots\wedge a_k, \label{equ_blade_A}\\
B &= b_1\wedge b_2\wedge\dots\wedge b_k,\label{equ_blade_B}
\end{align}
by applying Definition~\ref{def_blade}.
Notice that for blades of grade greater than 1, such factorizations are not unique!
For example, there may be many such sets $\{a_i\}_{i=1}^k$ of $k$ vectors
that we could come up with such that equation \eqref{equ_blade_A} is satisfied,
because there are many different possible sets of basis vectors we can choose that
span the vector sub-space represented by $A$.

Now comes the critical observation.  It is that, under a certain circumstance, we
can choose factorizations of $A$ and $B$ such that $a_k=b_k$.  Then, using
the distributive property of the outer product over addition, we have
\begin{equation}
C = (a_1\wedge a_2\wedge\dots\wedge a_{k-1}+b_1\wedge b_2\wedge\dots\wedge b_{k-1})\wedge c_k,
\end{equation}
where $c_k=a_k=b_k$.  What we've now done is reduce the problem of adding
$k$-blades to that of adding blades of grade $k-1$.  Inductively, we can say that we've
found a way to add $k$-blades, because ultimately, if our certain circumstance persists,
we will eventually reduce the problem of adding the two blades to that
of adding 1-blades (vectors), which thing is already defined by equation
\eqref{equ_vec_addition} of Definition~\ref{def_vec_space}.

So then what is this certain circumstance?  Recall that $A$ and $B$ represent
vector sub-spaces of $\V$.  Then isn't it clear that if these vector sub-spaces
non-trivially overlap, (share more than just the zero vector in common), then we can find
a non-zero vector $c_k$ that exists in both vector sub-spaces?  Given such a
vector $c_k$, we now need only write each of $A$ and $B$ in terms of this
vector, such that $a_k=c_k$ and $b_k=c_k$.

It is now clear that two blades $A$ and $B$ of grade $k>1$ fully add if and only if they
share a common vector sub-space of dimension $k-1$.  Notice that
two 1-blades (vectors), may represent disjoint 1-dimensional vector sub-spaces,
(with the exception of the zero vector), yet we can still add them by
equation \eqref{equ_vec_addition} of Definition~\ref{def_vec_space}.

\subsection{The Outer Product}

We needed the outer product to get going with blades, but we now
return to it to give it a closer look and a more formal treatment.

Intuitively, the outer product simply allows us to glue vectors
together to form blades.  What we hope to gain in this section
is some kind of geometric intuition about blades, and this will
help us learn more about the outer product.

Recall in Definition~\ref{def_blade} that the outer product is anti-commutative.
This means that the transposition of any two adjacent vectors in
equation \eqref{equ_blade} leaves $B$ unchanged, provided we negate the
entire outer product.  That is, we have
\begin{equation}\label{equ_adj_trans}
B = -v_1\wedge v_2\wedge\dots\wedge v_{i+1}\wedge v_i\wedge\dots\wedge v_k,
\end{equation}
where $1\leq i<k$.  We'll return to the geometric significance of this momentarily.

Considering $\V$ to be a Euclidean vector space, all vectors $v\in\V$ have
a Euclidean interpretation.  That is, we can think of $v$ as being a directed
line segment with arbitrary position.  Using this idea, we can extend the notion
of vector magnitude to blade magnitude in the following definition.
\begin{definition}\label{def_blade_mag}
Given a non-zero $k$-blade $B$, and letting $\{v_k\}_{i=1}^k$ be
an orthogonal basis for the vector sub-space of $\V$ represented
by $B$, we define the magnitude of $B$, denoted by $|B|$, as
\begin{equation}
|B| = |v_1||v_2|\dots |v_k|,
\end{equation}
where for each $1\leq i\leq k$, $|v_i|$ denotes the length of $v_i$.
\end{definition}
If $\{v_k\}_{k=1}^k$ is an orthogonal basis, then for any pair of vectors $a,b\in\{v_i\}_{i=1}^k$,
we have $a\cdot b=0$, which is to say that $a$ and $b$ are perpendicular to one another.
Clearly, $|B|$ is a $k$-dimensional hyper-volume.  That is, length if $k=1$, area if $k=2$, volume
if $k=3$, and so on.

But how do we know that such a basis as that given in Definition~\ref{def_blade_mag} always exists?
The answer is the Grahm-Schmidt orthogonalization process.  Notice that for any scalar $\lambda\in\R$,
we can rewrite $B$ in equation \eqref{equ_blade} as
\begin{equation}\label{equ_grahm_schmidt_step_form}
B = v_1\wedge v_2\wedge\dots\wedge(v_i+\lambda v_j)\wedge\dots\wedge v_k,
\end{equation}
where $1\leq i\leq k$, $1\leq j\leq k$ and $i\neq j$.  Every step of the Grahm-Schmidt
orthogonalization process, which process, for brevity, we'll leave as a research project for the reader,
is of the form of equation \eqref{equ_grahm_schmidt_step_form}, and in the end, produces
a factorization of $B$ in terms of an orthogonal basis.

The geometric take-home point of all this is that blades, under a geometric interpretation,
have arbitrary shape.  While a vector is always a line segment, we may visualize a blade of
grade greater than one
as an area in a plane or a volume in a hyper-plane, the shape of that area or volume being completely
arbitrary.  The defining characteristics of a $k$-blade are simply the orientation of the $k$-dimensional
hyper-volume, the amount of that hyper-volume, and another property known as handedness.
Handedness is a property
that manifests itself only in the comparison of two blades $A$ and $B$ of the same grade with
the same orientation
and magnitude.  In this case, if $\{a_i\}_{i=1}^k$ is a sequence of vectors such
that equation \eqref{equ_blade_A} is satisfied, then $B$ is given by equation
\eqref{equ_blade_B}, where $\{b_i\}_{i=1}^k$ is simply an even or odd permutation
of the vectors in $\{a_i\}_{i=1}^k$.  That is, there exists a bijective map between
$\{a_i\}_{i=1}^k$ and $\{b_i\}_{i=1}^k$.
Then, seeing that any permutation $\{b_i\}_{i=1}^k$ of $\{a_i\}_{i=1}^k$
is reached by performing a finite sequence of adjacent transpositions, this means that
by equation \eqref{equ_adj_trans}, $A$ and $B$ differ algebraicly at most by a sign.
That is, $A=\pm B$.  In the case that $A=B$, each have the same handedness,
while in the case that $A=-B$, they are said to have opposite handedness.

A last look at the outer product that summarizes all of the geometric characteristics of a $k$-blade
is given by
\begin{equation}\label{equ_blade_orthogonalized}
B = \left|\begin{array}{cccc}
b_1\cdot v_1 & b_1\cdot v_2 & \dots & b_1\cdot v_k \\
b_2\cdot v_1 & b_2\cdot v_2 & \dots & b_2\cdot v_k \\
\vdots & \vdots & \ddots & \vdots \\
b_k\cdot v_1 & b_k\cdot v_2 & \dots & b_k\cdot v_k
\end{array}\right|b_1\wedge b_2\wedge\dots\wedge b_k,
\end{equation}
where here, $B$ was originally written as it is in equation \eqref{equ_blade}, and
for each $1\leq i\leq k$, we have here rewritten $v_i$ in terms of the alternative basis $\{b_i\}_{i=1}^k$,
to get equation \eqref{equ_blade_orthogonalized}.  If $\{b_i\}_{i=1}^k$ is an orthonormal basis,
then we can take the absolute value of the determinant to be $|B|$, and its sign as an
indication of handedness relative to what we usually refer to as a right-handed system.
The orientation of $B$ is that of the unit $k$-blade $b_1\wedge b_2\wedge\dots\wedge b_k$.

\subsection{The Inner Product}

Thus far, we have already used the reader's knowledge of the inner product
between vectors taken from $\V$.  As a function, the inner product of vectors is
what we call a bilinear form.  How we define this bilinear form on $\V$ will determine
what is called the signature of our geometric algebra, because, as we'll see,
the inner product among blades of various grades is ultimately defined
in terms of the inner product among vectors.

Like the outer product, we define the inner product to be left and
right distributive over addition.  The commutativity of the inner
product, however, will very with the grade of the blades taken
in the product.  The inner product is not generally associative!

% break out formulas for inner products.  use recursive def with blades.

\subsection{Comparing the Inner and Outer Products}

In this section we want to build intuition about the inner and
outer products as performing operations opposite to one another.
That is, one operation, in a sense, undoes the other.
%We start with the ideas of the vector projection and vector rejection.
% part of vector in and out of blade.
%\begin{definition}
%
%\end{definition}

\subsection{The Geometric Product}

% inertible, zero product property

% juxtaposition

% versors.  psuedo-versors?

% reverse

% versors and outermorphisms

\end{document}