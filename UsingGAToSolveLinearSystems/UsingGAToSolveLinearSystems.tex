\documentclass{birkjour}

\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{amsthm}
\usepackage{graphicx}
%\usepackage{float}

\title{On The Use of Geometric Algebra\\to Solve Systems of Linear Equations}
\author{Spencer T. Parkin}
\address{3848 Harris Blvd.\\Ogden, UT  84408}
\email{spencer.parkin@proton.me}

\newcommand{\G}{\mathbb{G}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\grade}{\mbox{grade}}
\newcommand{\prl}{\parallel}
\newcommand{\prp}{\perp}

\begin{document}

\begin{abstract}
A common equation that arrises in science and engineering is as follows.
\begin{equation}\label{equ_matrix_form_of_system}
Mx=y
\end{equation}
Here, $M$ is a square matrix, and $x,y$ are vectors.  If $M$ is non-singular, the solution is simply
\begin{equation}\label{equ_matrix_algebra_solved}
x=M^{-1}y.
\end{equation}
This is quite satisfactory in the sense that matrix inverses are well understood, but the contention here is that
there is still something left wanting.  You see, a general matrix inverse requires a row-reduction algorithm, or
a formulation in terms of an adjugate matrix that seems to have unknown motivation.  In contrast, what we'll show in this paper is that a
system of linear equations has a straight-forward and clear formulation in terms of geometric algebra.
\end{abstract}

\maketitle

%\keywords{Geometric Algebra, Linear Algebra, Linear Transform}

In \cite{Hestenes84}, equation 1.21a, and in \cite{Hestenes91}, equation 2.16, a general formula for the inverse of a linear transform is given,
but its practical computation is unclear.  Our goal here will be to give such a formulation, but one that
is easier to compute.  We will not try to generalize a linear transformation to blades other than those of grade 1 (i.e., vectors.)

To that end, let $x,y$ be vectors taken from an $n$-dimensional vector space $\V$, and let the rows of $M$
be denoted by vectors $m_1$, $m_2$, and so on, to $m_n$, each taken from the same vector space $\V$.
Having done so, our system of linear equations can be expressed as
\begin{equation}\label{equ_linear_system}
\left\{
\begin{array}{cc}
m_1\cdot x &= y\cdot e_1 \\
m_2\cdot x &= y\cdot e_2 \\
\vdots & \vdots \\
m_n\cdot x &= y\cdot e_n
\end{array}
\right.
\end{equation}
Yes, in contrast with equation \eqref{equ_matrix_form_of_system}, this is now looking like geometric algebra; and no, it doesn't look as nice as matrix algebra,
but bare with me.  Note that $\{e_i\}_{i=1}^n$ are an orthonormal set of basis vectors for $\V$.

For a vector $a\in\V$ and a blade $B$ of grade $k\leq n$, we will now make use of the following identity.  (This identity is justified in the appendix.)
\begin{equation}\label{equ_useful_identity}
a\cdot B = -\sum_{i=1}^k(-1)^i (a\cdot b_i) \bigwedge_{j=1,j\neq i}^k b_j,
\end{equation}
where $B=\bigwedge_{i=1}^k b_i$, each $b_i\in\V$.  With this in hand, we may return to our
system of linear equations, and write
\begin{equation}
x\cdot\bigwedge_{i=1}^n m_i = -\sum_{i=1}^n (-1)^i (x\cdot m_i)\bigwedge_{j=1,j\neq i}^n m_j.
\end{equation}
Interestingly, although our unknown vector $x$ is used on both sides of the equation here, the right-hand side
is completely known.  To illustrate this, let's rewrite it with each equation in \eqref{equ_linear_system} substituted.
\begin{equation}\label{equ_substituted}
x\cdot\bigwedge_{i=1}^n m_i = -\sum_{i=1}^n (-1)^i (y\cdot e_i)\bigwedge_{j=1,j\neq i}^n m_j
\end{equation}
What's more, realize that
\begin{equation}
x\wedge\bigwedge_{i=1}^n m_i = 0,
\end{equation}
since clearly in $(n+1)$-blade formed from $\V$ is necessarily zero.  Thus, the inner product of \eqref{equ_substituted}
can be changed to a geometric product.  This allows us to isolate $x$ on the left-hand side by multiplying both sides
by an inverse as follows.
\begin{equation}
x = \left(-\sum_{i=1}^n (-1)^i (y\cdot e_i)\bigwedge_{j=1,j\neq i}^n m_j\right)\left(\bigwedge_{i=1}^n m_i\right)^{-1}
\end{equation}
Some notation now might help to clean this up.  We'll let $M=\bigwedge_{i=1}^n m_i$, and
\begin{equation}
M_i=\bigwedge_{j=1,j\neq i}^n m_j,
\end{equation}
as well as $y_i=y\cdot e_i$.  Our equation then becomes
\begin{equation}
x = \left(-\sum_{i=1}^n(-1)^i y_i M_i\right)M^{-1}.
\end{equation}
But since the grade of $M$ is $n$, we also know that $M$ is a psuedo-scalar of the geometric algebra generated by $\V$.
That is, $M$ is a scalar multiple of the unit psuedo-scalar $I$, or $M=|M|I$.  We therefore have
\begin{equation}
M^{-1} = \frac{1}{|M|}I^{-1} = \frac{(-1)^{n(n-1)/2}}{|M|}I.
\end{equation}
Therefore, if we let $M_i^{*}=M_iI$ denote the dual of $M_i$, then we finally arrive at
\begin{equation}\label{equ_ga_solved}
x = \frac{-(-1)^{n(n-1)/2}}{|M|}\sum_{i=1}^n (-1)^i y_i M_i^{*}.
\end{equation}
The reader may recognize here $|M|$ as being the determinant of $M$.  Equation \eqref{equ_ga_solved}
may be a bit more complicated than equation \eqref{equ_matrix_algebra_solved}, but it doesn't leave
any question about how to actually calculate $x$.  Interestingly, we see $x$ here is a certain linear
combination of the duals of each $M_i$.  This lets us at least try to visualize the geometric significance
of the result.

Let's now consider how we might use our result to find the general inverse of a linear transformation.
First, let's express the original linear transformation as follows.
\begin{equation}
f(x) = \sum_{i=1}^n (m_i\cdot x)e_i
\end{equation}
If we now let
\begin{equation}
a_i = \frac{-(-1)^{i+n(n-1)/2}}{|M|}M_i^{*},
\end{equation}
then we may write
\begin{equation}
f^{-1}(x)=\sum_{i=1}^n (x\cdot e_i)a_i.
\end{equation}
Here, we almost have the inverse function in the same form as the original function.
To get it there, let $A$ be a matrix with each row being $a_i$.  If we let $a_i'$ denote
each column of $A$, we can then write
\begin{equation}
f^{-1}(x)=\sum_{i=1}^n (a_i'\cdot x)e_i.
\end{equation}
And there we have it.  This is still not quite satisfactory, though, because we still had to use some
matrix trickery to get the result, but it's at least directly computable with minimal need for algorithms
beyond basic arithmatic.

\subsection*{Appendix}

Here we are going to derive the identity in equation \eqref{equ_useful_identity}.
Let $B=\bigwedge_{i=1}^k b_i$ be a $k$-vector, each $b_i\in\V$, and let $a\in\V$ as well.
We want to calculate $a\cdot B$.  Without loss of generality, we will assume that the set
of vectors $\{b_i\}_{i=1}^k$ forms an orthogonal basis.  That is, for every $i\neq j$, we
have $b_i\cdot b_j=0$.

We begin by realizing that
\begin{equation}
a\cdot B = (a_{\prp}+a_{\prl})\cdot B = a_{\prl}\cdot B,
\end{equation}
which is to say that only the part of $a$ in the space of $B$ contributes to the result.
And indeed, since $a_{\prl}\wedge B=0$, we may expression $a_{\prl}$ in terms
of the basis of $B$ as follows.
\begin{equation}
a_{\prl}=\sum_{i=1}^k (a_{\prl}\cdot b_i)b_i
\end{equation}
Putting all this together, we now have
\begin{equation}
a\cdot B = \sum_{i=1}^k (a_{\prl}\cdot b_i)b_i\cdot\bigwedge_{j=1}^k b_j
\end{equation}
With a bit of rearrangement, it is now easy to see that
\begin{equation}
a\cdot B = \sum_{i=1}^k (a_{\prl}\cdot b_i)b_i\cdot (-1)^{i-1}b_i\wedge\bigwedge_{j=1,j\neq i}^k b_j.
\end{equation}
And this simplifes to
\begin{equation}
a\cdot B = -\sum_{i=1}^k (-1)^i(a_{\prl}\cdot b_i)\bigwedge_{j=1,j\neq i}^k b_j.
\end{equation}
Lastly, simply realize that for all $i$, we have $a_{\prl}\cdot b_i=a\cdot b_i$, since $a_{\prp}\cdot b_i=0$,
and there we have it.

To be extra thorough here, let's examine the case that $B$ is not factored into an orthogonal basis.
Does the identity \eqref{equ_useful_identity} still hold?  We begin with the left-hand side, and
let $r\neq s$ when we write
\begin{equation}
a\cdot(b_1\wedge\dots\wedge(b_r+\lambda b_s)\wedge\dots\wedge b_k)=a\cdot B.
\end{equation}
Here, $\lambda$ is a scalar, and we see that replacing $b_r$ in $B$ with $b_r+\lambda b_s$ does not alter
the left-hand side of equation \eqref{equ_useful_identity}.  The idea is that any basis for $B$ can be reached from an orthogonal basis through
a sequence of shears after perhaps an initial rotation.  (Think of reversing the Grahm-Schmidt orthogonalization
process.)

Is the right-hand side preserved in a shear?
For some pair of integers $r\neq s$, let $B_i=\bigwedge_{j=1,j\neq i}^k b_i$, and then let $B_i'$ be $B_i$ with $b_r$ replaced with $b_r+\lambda b_s$.
Having done so, we see that $B_i=B_i'$ in all cases $i\neq s$.  We then must examine the following expression.
\begin{equation}
(a\cdot(b_r+\lambda b_s))B_r+(-1)^{|r-s|}(a\cdot b_s)B_s'
\end{equation}
Note that the sign of the first term is chosen arbitrarily to be positive, while the sign of the other term must differ by $(-1)^{|r-s|}$.
This expression becomes
\begin{equation}
(a\cdot b_r)B_r + \lambda (a\cdot b_s)B_r + (-1)^{|r-s|}[(a\cdot b_s)B_s-\lambda(-1)^{|r-s|}(a\cdot b_s)B_r].
\end{equation}
Finally, we get some cancelation, and the expression becomes
\begin{equation}
(a\cdot b_r)B_r+(-1)^{|r-s|}(a\cdot b_s)B_s,
\end{equation}
showing that the right-hand side of equation \eqref{equ_useful_identity} is not alterned by a shear either.

\begin{thebibliography}{2}

\bibitem{Hestenes84}
G. Sobczyk, D. Hestenes
{\it Clifford Algebra to Geometric Calculus}.  D. Reidel Publishing Company, 1984.

\bibitem{Hestenes91}
D. Hestenes
{\it The Design of Linear Algebra and Geometry}.  Acta Applicandae Mathematicae, Kluwer Academic Publishers 23: 65-93, 1991.

\end{thebibliography}

\end{document}