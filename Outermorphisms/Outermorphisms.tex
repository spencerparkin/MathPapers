\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{float}

\title{Outermorphisms}
\author{Spencer T. Parkin}

\newcommand{\G}{\mathbb{G}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\uf}{\underline{f}}
\newcommand{\of}{\overline{f}}
\newcommand{\grade}{\mbox{grade}}

\begin{document}
\maketitle

Letting $\G$ be a geometric algebra generated by a vector space $\V$,
and letting $f:\V\to\V$ be a linear transform defined on that vector space, there exists an extension $\uf$ of $f$ to
all of $\G$ given by $\uf(a)=f(a)$ whenever $a\in \V$, and whenever $\lambda\in\R$ and $A,B\in G$, we have
\begin{align}
\uf(A+B) &= \uf(A) + \uf(B), \\
\uf(\lambda A) &= \lambda\uf(A), \\
\uf(\lambda) &= \lambda, \\
\uf(A\wedge B) &= \uf(A)\wedge\uf(B).
\end{align}
Clearly $\uf$ is grade perserving.  That is, $\uf(\langle A\rangle_i)=\langle\uf(A)\rangle_i$.
It follows that $\uf(I)=\lambda I$, where $I$ is the unit pseudo-scalar of $\G$.
In this case, we define $\det f=\lambda$.  That is, we define
\begin{equation}
\det\uf = I^{-1}\uf(I).
\end{equation}
(Notice that $\det\uf$ is an eigen-value for the eigen-blade $I$.)
Hestenes then defines the adjoint or transpose $\of$ of $\uf$ as being implicitly given by
\begin{equation}
\langle\of(A)B\rangle_0 = \langle A\uf(B)\rangle_0.\label{equ_adjoint}
\end{equation}
From this it is not at all obvious to me that $\of$ is an outermorphism, or even a linear transform
that can be extended to an outermorphism.  Assuming it is, however, it is easy to show that it
has the same determinant as $\uf$.
\begin{equation}
\det\uf=\langle I^{-1}\uf(I)\rangle_0=\langle\of(I^{-1})I\rangle_0=\langle\of(I)I^{-1}\rangle_0=\langle I^{-1}\of(I)\rangle_0=\det\of
\end{equation}
Then, citing a references I don't have access to, Hestenes claims that from all this he can derive the following identity, provided $\grade(A)\leq\grade(B)$.
\begin{equation}
\uf(\of(A)\cdot B)=A\cdot\uf(B)\label{equ_blah}
\end{equation}
Notice that this is clearly consistent with equation \eqref{equ_adjoint} as far as scalars go.

Now, realizing that the inner and geometric products are interchangable when one operand is a pseudo-scalar,
we can use equation \eqref{equ_blah} to find that
\begin{equation}
A = \frac{AI^{-1}\uf(I)}{\det f} = \frac{\uf(\of(AI^{-1})I)}{\det f} = \frac{\uf(\of(AI)I^{-1})}{\det f},
\end{equation}
provided, of course, that $\det f\neq 0$.  Finally, we see from this that
\begin{equation}
\uf^{-1}(A) = \uf^{-1}\left(\frac{\uf(\of(AI)I^{-1})}{\det f}\right) = \frac{\of(AI)I^{-1}}{\det f}.\label{equ_inv}
\end{equation}

This is really interesting to me, although I still have no idea how to use it in practice to, say, calculate
the inverse of a matrix.  This is certainly a more elegant formulation of the inverse of a linear transformation
than the one presented in my linear algebra textbook.

Returning to \eqref{equ_blah}, a special case of this is easy to prove.

\begin{align}
 & \uf\left(\of(a)\cdot\bigwedge_{i=1}^n b_i\right) \\
=& \uf\left(-\sum_{i=1}^n(-1)^i (\of(a)\cdot b_i)\bigwedge_{j=1,j\neq i}^n b_j\right) \\
=& -\sum_{i=1}^n(-1)^i (\of(a)\cdot b_i)\bigwedge_{j=1,j\neq i}^n \uf(b_j) \\
=& -\sum_{i=1}^n(-1)^i (a\cdot\uf(b_i))\bigwedge_{j=1,j\neq i}^n \uf(b_j) \\
=& a\cdot\bigwedge_{i=1}^n\uf(b_i) \\
=& a\cdot \uf\left(\bigwedge_{i=1}^n b_i\right)
\end{align}
Then, if $\grade(A)\leq\grade(B)$ and $A=\bigwedge_{i=1}^m a_i$ and $B=\bigwedge_{i=1}^n b_j$, we can use
\begin{equation}
A\cdot B = \bigwedge_{i=1}^{m-1} a_i\cdot\left(a_m\cdot \bigwedge_{i=1}^n b_i\right)
\end{equation}
to show, by induction, that equation \eqref{equ_blah} holds in the case of blades.  We have
\begin{align}
 =& \uf\left(\of\left(\bigwedge_{i=1}^m a_i\right)\cdot\bigwedge_{i=1}^n b_i\right) \\
 =& \uf\left(\of\left(\bigwedge_{i=1}^{m-1} a_i\right)\cdot\left(\of(a_m)\cdot\bigwedge_{i=1}^n b_i\right)\right)\label{equ_induct_before} \\
 =& \bigwedge_{i=1}^{m-1} a_i\cdot\uf\left(\of(a_m)\cdot\bigwedge_{i=1}^n b_i\right)\label{equ_induct_after} \\
 =& \bigwedge_{i=1}^{m-1} a_i\cdot\left(a_m\cdot\uf\left(\bigwedge_{i=1}^n b_i\right)\right) \\
 =& \left(\bigwedge_{i=1}^m a_i\right)\cdot\uf\left(\bigwedge_{i=1}^n b_i\right).
\end{align}
Notice that the inductive hypothesis was used going from equation \eqref{equ_induct_before} to \eqref{equ_induct_after}.

To finish a proof of equation \eqref{equ_blah} for multivectors in general, I believe all we need to do is cite the distributivity of outermorphisms
over addition.

Returning to equation \eqref{equ_inv}, calculating the inverse of a matrix may involving
calculating $\uf^{-1}(e_i)$ for each basis vectors $e_i$ as all linear transformations are
determined by how they transform a basis of the vector space.  Hestenes found an explicit
form for the adjoint $\of$ using some calculus.  Perhaps I should look there.

\end{document}